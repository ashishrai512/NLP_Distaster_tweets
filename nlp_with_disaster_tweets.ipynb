{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import decomposition\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "\n",
    "import geopandas as gpd\n",
    "from geotext import GeoText\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test.isna().sum())\n",
    "print(train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>54</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>@PhDSquares #mufc they've built so much hype a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>55</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>World Wide!!</td>\n",
       "      <td>INEC Office in Abia Set Ablaze - http://t.co/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>56</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>57</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Paranaque City</td>\n",
       "      <td>Ablaze for you Lord :D</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>59</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                       location  \\\n",
       "31  48  ablaze                     Birmingham   \n",
       "32  49  ablaze  Est. September 2012 - Bristol   \n",
       "33  50  ablaze                         AFRICA   \n",
       "34  52  ablaze               Philadelphia, PA   \n",
       "35  53  ablaze                     London, UK   \n",
       "36  54  ablaze                       Pretoria   \n",
       "37  55  ablaze                   World Wide!!   \n",
       "38  56  ablaze                            NaN   \n",
       "39  57  ablaze                 Paranaque City   \n",
       "40  59  ablaze                 Live On Webcam   \n",
       "\n",
       "                                                 text  target  \n",
       "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "32  We always try to bring the heavy. #metal #RT h...       0  \n",
       "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "34                 Crying out for more! Set me ablaze       0  \n",
       "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n",
       "36  @PhDSquares #mufc they've built so much hype a...       0  \n",
       "37  INEC Office in Abia Set Ablaze - http://t.co/3...       1  \n",
       "38  Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...       1  \n",
       "39                             Ablaze for you Lord :D       0  \n",
       "40  Check these out: http://t.co/rOI2NSmEJJ http:/...       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[~train['keyword'].isna()][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag(text):\n",
    "    text = text.lower()\n",
    "    t = re.findall(r'#[a-z]+',text)\n",
    "    t = \" \".join(t)\n",
    "    t = re.sub(r'#','',t)\n",
    "    return t\n",
    "    \n",
    "# def cities(text):\n",
    "#     text= text.lower()\n",
    "# #     text = re.sub(r'#',' ',text)\n",
    "# #     t = \"\".join(text)\n",
    "#     places= GeoText(text)\n",
    "#     cities = list(places.cities)+list(places.countries)\n",
    "#     return cities\n",
    "    \n",
    "train['hash_words'] = train['text'].apply(lambda x: hashtag(x))\n",
    "#train['cities'] = train['text'].apply(lambda x: cities(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global hash_target_1,hash_target_0\n",
    "# hash_target_1 = ' '\n",
    "# for t in train[train['target']==1].hash_words:\n",
    "#     hash_target_1+=t+' '\n",
    "# hash_target_1 =[t for t in hash_target_1.split()]\n",
    "\n",
    "# hash_target_0 = ' '\n",
    "# for t in train[train['target']==0].hash_words:\n",
    "#     hash_target_0+=t+' '\n",
    "# hash_target_0 =[t for t in hash_target_0.split()]\n",
    "\n",
    "\n",
    "\n",
    "# def all_hash_1(text):\n",
    "#     text = text.lower()\n",
    "#     text = [t for t in text.split()]\n",
    "#     t = [t for t in text if t in hash_target_1]\n",
    "#     return \" \".join(t)\n",
    "# def all_hash_0(text):\n",
    "#     text = text.lower()\n",
    "#     text = [t for t in text.split()]\n",
    "#     t = [t for t in text if t in hash_target_0]\n",
    "#     return \" \".join(t)\n",
    "\n",
    "# train['all_hash_1']= train['text'].apply(lambda x: all_hash_1(x))\n",
    "# train['all_hash_0']= train['text'].apply(lambda x: all_hash_0(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1=\"\"\n",
    "for t in train[train['target']==1].hash_words:\n",
    "    target_1+=t+' '\n",
    "\n",
    "target_0=\"\"\n",
    "for t in train[train['target']==0].hash_words:\n",
    "    target_0+=t+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earthquake   wildfires alaska wildfires rockyfire cafire wildfires flood disaster     flooding raining flooding florida tampabay tampa flood we breaking  africanbaze  bridgetown     kurds diyala  california climate energy   nashvilletraffic santaclara bayarea traffic        truckcrash fortworth ashville traffic    manchester traffic       breaking hagerstown whag  bahrain news        horrible accident watchthevideo   kca votejkt rip binladen mlb man airport airplane aircraft aeroplane runway accident freaky    crash aircraft airplane pilot death accident carfest       omg rip airplane accident jetengine turbojet boing g      rodkiai  emsne yugvani    news til dna      reuters    worldnews worldnews                   gilbert     internetradio collegeradi     storm apocalypse   pbban   armageddon brics roberts russia   directioners      newsintweets       kisii countynews    kisii countynews   lgbt  lesbian       arsonist headlines nightbeat sanfrancisco newyork  nativehuman myreligion  '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_1[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                metal rt   mufc  nsfw       nsfw         nowplaying edm   personalinjury solicitor otleyhour stlouis caraccidentlawyer        arrestpastornganga   dubstep trapmusic dnb edm dance ices  dubstep trapmusic dnb edm dance ices    growingupspoiled    dubstep trapmusic dnb edm dance ices dubstep trapmusic dnb edm dance ices dubstep trapmusic dnb edm dance ices  dubstep trapmusic dnb edm dance ices dubstep trapmusic dnb edm dance ices wisdomwed lifehacks  silverwood aftershock  book   now wdyouth biblestudy                 justsaying randomthought     ems paramedics ambulance            mets              fantasticfour fant      lgm            gilbert gilbert az wildhorses tantonationalforest   saltriverwildhorses     sciencefiction     warmbodies            etcpb    pbban pbban doublecups armageddon      love truelove romance voodoo seduction astrology rtrrt lotz apocalypse armageddon  eonlinechat   love truelove romance voodoo seduction astrology rtrrt lotz apocalypse armagedd'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_0[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n",
    "    text = re.sub('[~!@#$\\'%&*()-?=]+','',text)\n",
    "    text = [t for t in text.split() if len(t)>2]\n",
    "    text= \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_clean\"] = train['text'].apply(lambda x: text_clean(x))\n",
    "test[\"text_clean\"] = test['text'].apply(lambda x: text_clean(x))\n",
    "\n",
    "train.drop(['text'],axis=1,inplace=True)\n",
    "test.drop(['text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>target</th>\n",
       "      <th>hash_words</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>our deeds are the reason this earthquake may a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>all residents asked shelter place are being no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfires</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>alaska wildfires</td>\n",
       "      <td>just got sent this photo from ruby alaska smok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  target        hash_words  \\\n",
       "0   1     NaN      NaN       1        earthquake   \n",
       "1   4     NaN      NaN       1                     \n",
       "2   5     NaN      NaN       1                     \n",
       "3   6     NaN      NaN       1         wildfires   \n",
       "4   7     NaN      NaN       1  alaska wildfires   \n",
       "\n",
       "                                          text_clean  \n",
       "0  our deeds are the reason this earthquake may a...  \n",
       "1                 forest fire near ronge sask canada  \n",
       "2  all residents asked shelter place are being no...  \n",
       "3  people receive wildfires evacuation orders cal...  \n",
       "4  just got sent this photo from ruby alaska smok...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.text_clean\n",
    "y=train.target\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 2), stop_words = 'english')\n",
    "\n",
    "\n",
    "#train and validation for TF-IDF\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "#train and validation for Count Vectorizer\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808,"
     ]
    }
   ],
   "source": [
    "# Logistic Regression using TF-IDF as input vector\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(xtrain_tfv,ytrain)\n",
    "score = accuracy_score(yvalid,clf.predict(xvalid_tfv))\n",
    "print(round(score,3),end=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804,"
     ]
    }
   ],
   "source": [
    "# Logistic Regression using count_vectorizer as input vector\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(xtrain_ctv,ytrain)\n",
    "score = accuracy_score(yvalid,clf.predict(xvalid_ctv))\n",
    "print(round(score,3),end=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Light GBM Classifier  using CountVectorizer ngram(1,2)\n",
    "clf = lgb.LGBMClassifier(max_depth=7, n_estimators=1000, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv,ytrain)\n",
    "score = accuracy_score(yvalid,clf.predict(xvalid_ctv))\n",
    "print(round(score,3),end=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic Regression with skfold using TF-IDF ngram(1,2)\n",
    "# skf = StratifiedKFold(n_splits=10)\n",
    "# pred_test =np.zeros((len(test),1))\n",
    "# cv_score=[]\n",
    "# for train_index,test_index in skf.split(X,y):\n",
    "#     xtrain,xval = X.iloc[train_index],X.iloc[test_index]\n",
    "#     ytrain,yval = y.iloc[train_index],y.iloc[test_index]\n",
    "    \n",
    "#     tfv.fit(list(xtrain) + list(xval)+list(test.text_clean))\n",
    "#     xtrain_tfv =  tfv.transform(xtrain) \n",
    "#     xval_tfv = tfv.transform(xval)\n",
    "#     test_tfv = tfv.transform(test.text_clean)\n",
    "    \n",
    "    \n",
    "#     clf = LogisticRegression(C=1)\n",
    "#     clf.fit(xtrain_tfv,ytrain)\n",
    "#     score = accuracy_score(yval,clf.predict(xval_tfv))\n",
    "#     cv_score.append(score)\n",
    "#     print(round(score,3),end=\",\")\n",
    "\n",
    "#     pred_test += clf.predict_proba(test_tfv)[:,1].reshape(-1,1)\n",
    "# pred_test=pred_test/10 \n",
    "# print(\"\\n\",round(np.mean(cv_score),3))\n",
    "# test['lr_tfv']=pred_test\n",
    "# del(xtrain_tfv,xval_tfv,test_tfv,xtrain,ytrain,xval,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic Regression with skfold using count_vectorizer n_gram(1,2)\n",
    "# skf = StratifiedKFold(n_splits=10)\n",
    "# pred_test =np.zeros((len(test),1))\n",
    "# cv_score=[]\n",
    "# for train_index,test_index in skf.split(X,y):\n",
    "#     xtrain,xval = X.iloc[train_index],X.iloc[test_index]\n",
    "#     ytrain,yval = y.iloc[train_index],y.iloc[test_index]\n",
    "    \n",
    "#     ctv.fit(list(xtrain) + list(xval)+list(test.text_clean))\n",
    "#     xtrain_ctv =  ctv.transform(xtrain) \n",
    "#     xval_ctv = ctv.transform(xval)\n",
    "#     test_ctv = ctv.transform(test.text_clean)\n",
    "    \n",
    "    \n",
    "#     clf = LogisticRegression(C=1)\n",
    "#     clf.fit(xtrain_ctv,ytrain)\n",
    "#     score = accuracy_score(yval,clf.predict(xval_ctv))\n",
    "#     cv_score.append(score)\n",
    "#     print(round(score,3),end=\",\")\n",
    "\n",
    "#     pred_test += clf.predict_proba(test_ctv)[:,1].reshape(-1,1)\n",
    "# pred_test=pred_test/10 \n",
    "# print(\"\\n\",round(np.mean(cv_score),3))\n",
    "# test['lr_ctv']=pred_test\n",
    "# del(xtrain_ctv,xval_ctv,test_ctv,xtrain,ytrain,xval,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Light GBM Classifier with skfold using TF-IDF ngram(1,2)\n",
    "# clf = lgb.LGBMClassifier(max_depth=7, n_estimators=1000, colsample_bytree=0.8, \n",
    "#                         subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=10)\n",
    "# pred_test =np.zeros((len(test),1))\n",
    "# cv_score=[]\n",
    "# for train_index,test_index in skf.split(X,y):\n",
    "#     xtrain,xval = X.iloc[train_index],X.iloc[test_index]\n",
    "#     ytrain,yval = y.iloc[train_index],y.iloc[test_index]\n",
    "    \n",
    "#     tfv.fit(list(xtrain) + list(xval)+list(test.text_clean))\n",
    "#     xtrain_tfv =  tfv.transform(xtrain) \n",
    "#     xval_tfv = tfv.transform(xval)\n",
    "#     test_tfv = tfv.transform(test.text_clean)\n",
    "    \n",
    "#     clf.fit(xtrain_tfv.tocsc(),ytrain)\n",
    "#     score = accuracy_score(yval,clf.predict(xval_tfv.tocsc()))\n",
    "#     cv_score.append(score)\n",
    "#     print(round(score,3),end=\",\")\n",
    "\n",
    "#     pred_test += clf.predict_proba(test_tfv.tocsc())[:,1].reshape(-1,1)\n",
    "# pred_test=pred_test/10 \n",
    "# print(\"\\n\",round(np.mean(cv_score),3))\n",
    "# test['lgbm_tfv']=pred_test\n",
    "# del(xtrain_tfv,xval_tfv,test_tfv,xtrain,ytrain,xval,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Light GBM Classifier with skfold using count vectorizer ngram(1,2)\n",
    "# clf = lgb.LGBMClassifier(max_depth=7, n_estimators=1000, colsample_bytree=0.8, \n",
    "#                         subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=10)\n",
    "# pred_test =np.zeros((len(test),1))\n",
    "# cv_score=[]\n",
    "# for train_index,test_index in skf.split(X,y):\n",
    "#     xtrain,xval = X.iloc[train_index],X.iloc[test_index]\n",
    "#     ytrain,yval = y.iloc[train_index],y.iloc[test_index]\n",
    "    \n",
    "#     ctv.fit(list(xtrain) + list(xval)+list(test.text_clean))\n",
    "#     xtrain_ctv =  ctv.transform(xtrain) \n",
    "#     xval_ctv = ctv.transform(xval)\n",
    "#     test_ctv = ctv.transform(test.text_clean)\n",
    "    \n",
    "    \n",
    "#     clf.fit(xtrain_ctv.tocsc(),ytrain)\n",
    "#     score = accuracy_score(yval,clf.predict(xval_ctv.tocsc()))\n",
    "#     cv_score.append(score)\n",
    "#     print(round(score,3),end=\",\")\n",
    "\n",
    "#     pred_test += clf.predict_proba(test_ctv.tocsc())[:,1].reshape(-1,1)\n",
    "# pred_test=pred_test/10 \n",
    "# print(\"\\n\",round(np.mean(cv_score),3))\n",
    "# test['lgbm_ctv']=pred_test\n",
    "# del(xtrain_ctv,xval_ctv,test_ctv,xtrain,ytrain,xval,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.expanduser('~/Desktop/glove.6B/glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(xtrain))\n",
    "\n",
    "x_tr_seq  = tokenizer.texts_to_sequences(xtrain) \n",
    "x_val_seq = tokenizer.texts_to_sequences(xvalid)\n",
    "test_seq = tokenizer.texts_to_sequences(test.text_clean)\n",
    "\n",
    "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=100)\n",
    "x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n",
    "test_seq = pad_sequences(test_seq,maxlen=100)\n",
    "\n",
    "size_of_vocabulary=len(tokenizer.word_index) + 1\n",
    "print(size_of_vocabulary)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((size_of_vocabulary, 100))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Embedding(size_of_vocabulary,100,weights=[embedding_matrix],input_length=100,trainable=False)) \n",
    "\n",
    "\n",
    "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(64,activation='relu')) \n",
    "model.add(Dense(1,activation='sigmoid')) \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
    "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
    "\n",
    "history = model.fit(np.array(x_tr_seq),np.array(ytrain),\\\n",
    "                    batch_size=100,epochs=100,\n",
    "                    validation_data=(np.array(x_val_seq),np.array(yvalid)),\\\n",
    "                    verbose=1,callbacks=[es,mc])\n",
    "\n",
    "pred = model.predict(test_seq)\n",
    "test['glove_keras'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = (test['lr_tfv']+test['lgbm_tfv'])/2\n",
    "temp = test.copy()\n",
    "temp['target'] = np.where(temp['target']>=0.5,1,0)\n",
    "temp.index=temp.id\n",
    "temp = temp[['target']]\n",
    "temp.to_csv('try2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
